{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projet Deep Learning II - DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.DNN import DNN\n",
    "from models.DBN import DBN\n",
    "from models.RBM import RBM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement de la donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to transforms.PILToTensor() but binarizes the image\n",
    "from torchvision.utils import  _log_api_usage_once\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "class PILToBinTensor:\n",
    "    \"\"\"Convert a PIL Image to a tensor of the same type - this does not scale values.\n",
    "\n",
    "    This transform does not support torchscript.\n",
    "\n",
    "    Converts a PIL Image (H x W x C) to a Tensor of shape (C x H x W).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        _log_api_usage_once(self)\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        .. note::\n",
    "\n",
    "            A deep copy of the underlying array is performed.\n",
    "\n",
    "        Args:\n",
    "            pic (PIL Image): Image to be converted to tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        tensor = F.pil_to_tensor(pic)\n",
    "        tensor[tensor <= 127] = 0\n",
    "        tensor[tensor > 127] = 1\n",
    "        return tensor\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset from yann.lecun.com\n",
    "mnist_trainset = datasets.MNIST(root='./data/', train=True, transform=PILToBinTensor(), download=True)\n",
    "mnist_testset = datasets.MNIST(root='./data/', train=False, transform=PILToBinTensor(), download=False)\n",
    "\n",
    "#create data loader\n",
    "max_mnist_size = 60000\n",
    "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0] \n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset_reduced, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "# download test dataset\n",
    "max_mnist_size = 10000\n",
    "mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0] \n",
    "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset_reduced, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = mnist_trainset_reduced.dataset.data.shape[1]\n",
    "n_cols = mnist_trainset_reduced.dataset.data.shape[2]\n",
    "n_channels = 1\n",
    "n_pixels = n_rows*n_cols\n",
    "classification_size = 10\n",
    "img_shape = (n_rows, n_cols, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack data and labels\n",
    "#train data\n",
    "train_data, train_labels,train_clear_label = [], [], []\n",
    "for data, label in mnist_train_loader:\n",
    "    train_data.append(data.reshape(-1,n_rows * n_cols * n_channels))\n",
    "    train_labels.append(torch.nn.functional.one_hot(label, num_classes=classification_size))\n",
    "    train_clear_label.append(label)\n",
    "train_data = torch.cat(train_data).numpy()\n",
    "train_labels = torch.cat(train_labels).numpy()\n",
    "train_clear_label = torch.cat(train_clear_label).numpy()\n",
    "\n",
    "#test data\n",
    "\n",
    "# unpack data and labels\n",
    "#test data\n",
    "test_data, test_labels, test_clear_label = [], [], []\n",
    "for data, label in mnist_test_loader:\n",
    "    test_data.append(data.reshape(-1,n_rows * n_cols * n_channels))\n",
    "    test_labels.append(torch.nn.functional.one_hot(label, num_classes=classification_size))\n",
    "    test_clear_label.append(label)\n",
    "test_data = torch.cat(test_data).numpy()\n",
    "test_labels = torch.cat(test_labels).numpy()\n",
    "test_clear_label = torch.cat(test_clear_label).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations_mnist = 200\n",
    "num_iterations = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test DNN en variant le nombre de couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_couches = [1,2,3,4,5,6,7]\n",
    "dnn_pretrain_accuracy = []\n",
    "dnn_random_accuracy = []\n",
    "for i in n_couches :\n",
    "    couches_dnn = [n_pixels]\n",
    "    for _ in range(i):\n",
    "        couches_dnn.append(200)\n",
    "    dnn_pretrain = DNN()\n",
    "    dnn_pretrain.init_DNN(couches_dnn, classification_size)\n",
    "    dnn_pretrain.pretrain_DNN(num_iterations, learning_rate, batch_size, train_data)\n",
    "    dnn_pretrain.retropropagation(train_data, train_labels, learning_rate, batch_size, num_iterations_mnist)\n",
    "    dnn_pretrain_accuracy.append(dnn_pretrain.test_DNN(test_data,test_labels))\n",
    "\n",
    "    dnn_random = DNN()\n",
    "    dnn_random.init_DNN(couches_dnn, classification_size)\n",
    "    dnn_random.retropropagation(train_data, train_labels, learning_rate, batch_size, num_iterations_mnist)\n",
    "    dnn_random_accuracy.append(dnn_pretrain.test_DNN(test_data,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_pretrain_error = 1 - np.array(dnn_pretrain_accuracy)\n",
    "dnn_random_error = 1 - np.array(dnn_random_accuracy)\n",
    "plt.plot(n_couches,dnn_pretrain_error,label = \"DNN pré-entrainé\")\n",
    "plt.scatter(n_couches,dnn_pretrain_error)\n",
    "plt.plot(n_couches,dnn_random_error,label = \"DNN non pré-entrainé\")\n",
    "plt.scatter(n_couches,dnn_random_error)\n",
    "plt.xlabel(\"nombre de couches\")\n",
    "plt.ylabel(\"taux d'erreur\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test DNN en variant le nombre de neurones par couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurones = [100,300,500,700]\n",
    "dnn_pretrain_accuracy = []\n",
    "dnn_random_accuracy = []\n",
    "for n_neurone in n_neurones :\n",
    "    couches_dnn = [n_pixels,n_neurone,n_neurone]\n",
    "    dnn_pretrain = DNN()\n",
    "    dnn_pretrain.init_DNN(couches_dnn, classification_size)\n",
    "    dnn_pretrain.pretrain_DNN(num_iterations, learning_rate, batch_size, train_data)\n",
    "    dnn_pretrain.retropropagation(train_data, train_labels, learning_rate, batch_size, num_iterations_mnist)\n",
    "    dnn_pretrain_accuracy.append(dnn_pretrain.test_DNN(test_data,test_labels))\n",
    "\n",
    "    dnn_random = DNN()\n",
    "    dnn_random.init_DNN(couches_dnn, classification_size)\n",
    "    dnn_random.retropropagation(train_data, train_labels, learning_rate, batch_size, num_iterations_mnist)\n",
    "    dnn_random_accuracy.append(dnn_pretrain.test_DNN(test_data,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_pretrain_error = 1 - np.array(dnn_pretrain_accuracy)\n",
    "dnn_random_error = 1 - np.array(dnn_random_accuracy)\n",
    "plt.plot(n_neurones,dnn_pretrain_error,label = \"DNN pré-entrainé\")\n",
    "plt.scatter(n_neurones,dnn_pretrain_error)\n",
    "plt.plot(n_neurones,dnn_random_error,label = \"DNN non pré-entrainé\")\n",
    "plt.scatter(n_neurones,dnn_random_error)\n",
    "plt.xlabel(\"nombre de neuronnes\")\n",
    "plt.ylabel(\"taux d'erreur\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test DNN en variant le nombre de données d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_donnees_entrainement = [200, 1000, 3000, 7000, 10000, 30000, 60000]\n",
    "dnn_pretrain_accuracy = []\n",
    "dnn_random_accuracy = []\n",
    "for n_entrainement in n_donnees_entrainement :\n",
    "    couches_dnn = [n_pixels,n_neurone,n_neurone]\n",
    "    dnn_pretrain = DNN()\n",
    "    dnn_pretrain.init_DNN(couches_dnn, classification_size)\n",
    "    dnn_pretrain.pretrain_DNN(num_iterations, learning_rate, batch_size, train_data[0:n_entrainement])\n",
    "    dnn_pretrain.retropropagation(train_data[0:n_entrainement], train_labels[0:n_entrainement], learning_rate, batch_size, num_iterations_mnist)\n",
    "    dnn_pretrain_accuracy.append(dnn_pretrain.test_DNN(test_data,test_labels))\n",
    "\n",
    "    dnn_random = DNN()\n",
    "    dnn_random.init_DNN(couches_dnn, classification_size)\n",
    "    dnn_random.retropropagation(train_data[0:n_entrainement], train_labels[0:n_entrainement], learning_rate, batch_size, num_iterations_mnist)\n",
    "    dnn_random_accuracy.append(dnn_pretrain.test_DNN(test_data,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_pretrain_error = 1 - np.array(dnn_pretrain_accuracy)\n",
    "dnn_random_error = 1 - np.array(dnn_random_accuracy)\n",
    "plt.plot(n_donnees_entrainement,dnn_pretrain_error,label = \"DNN pré-entrainé\")\n",
    "plt.scatter(n_donnees_entrainement,dnn_pretrain_error)\n",
    "plt.plot(n_donnees_entrainement,dnn_random_error,label = \"DNN non pré-entrainé\")\n",
    "plt.scatter(n_donnees_entrainement,dnn_random_error)\n",
    "plt.xlabel(\"nombre de données d'entraînement\")\n",
    "plt.ylabel(\"taux d'erreur\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
